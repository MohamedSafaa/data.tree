---
title: "ID3 Classification using data.tree"
author: "Christoph Glur"
date: '`r Sys.Date()`'
output:
  html_document:
    includes:
      before_body: ID3.banner.html
    theme: cerulean
    toc: yes
    toc_depth: 3
---

<!--
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Example of using data.tree for ID3 classification}
  %\SweaveUTF8
-->


# Introduction

## About this Vignette

This vignette provides a stylized example of the capabilities of the data.tree package. 
The code for this vignette was written in less than an hour. This is possible because, thanks to the data.tree package,
the implementation of the training algorithm follows the algorithm's pseudo code almost line by line.

## Why Mushrooms? (trivia)

While preparing this example, I asked my nine year old daughter: "Ana√Øs, imagine you have a big basket full of mushrooms. How would you go about finding out which ones are poisonous?"

"Dad, why would I want to know?", she replies rolling her eyes. "No matter if they are poisonous or not, they are _dis-gust-ing_."

"OK, fair enough", I sigh, "Wrong daughter." 

I try my luck with Naomi, the seven year old: 

"Naomi, how could you find out if a mushroom is poisonous?", I ask her. 

"Well", she says. "I'd just eat it. If it makes me puke, it's poisonous." 

"But, common, there must be a better way! Think, please!"

"Think? Well, ask mom then, that's _her_ specialty!", she groans.

So I do.

"Where did you buy them?", my wife asks back. 

"Good question, but ... it doesn't matter." 

"No Sir, it _does_ matter!"

"Why?", I ask dubiously.

"I'd have to to know if they are _organic_, obviously", replies my personal schoolmaster.

"Eh, why? ... I don't know... well, whatever, ... so, fine, assume I found them in the forest, so they are all organic.", I reply in exasperation.

"Aha!", she jubilates triumphantly, "that makes it easy! If they are organic, they simply _must_ be good for you. Hence they simply _cannot_ be poisonous!" 

Quod erat whatever.

## Why ID3

If you are not a blind believer in organic food, and if you are older than 9, there is a non-zero probability that you'd tackle this problem differently. Still, there are dozens of different methods. Yet, chances are you'd come across a class of models that are called [classification trees](http://en.wikipedia.org/wiki/Decision_tree_learning). These models let you *classify* observations (e.g. things, outcomes) according to the observations' qualities, called *attributes*. Essentially, all of these models consist of creating a *tree*, where each *node* acts as a *router*. You insert your mushroom at the *root* of the tree, and then, depending on the mushroom's *attributes* (size, points, color, etc.), you follow along a different *path*, until a *leaf* node spits out your mushroom, together with a prediction on its edibility. 

You might already have guessed that there are two different steps involved in using such a model: *training* (i.e. constructing the tree), and *predicting* (i.e. using the tree to predict whether a given mushroom is poisonous). This vignette provides code to do both, using one of the very early algorithms to classify data according to discrete attributes: [ID3](http://en.wikipedia.org/wiki/ID3_algorithm).

We will not go into more details about ID3. You will find lots of documentation on this and more refined algorithms on the internet. For example, lecture notes that build on similar data can be found [here](http://www.uni-weimar.de/medien/webis/teaching/lecturenotes/machine-learning/unit-en-decision-trees-algorithms.pdf).

Also, be assured that this example is by no means meant to be used in the real world. It is overly simplistic, and far too little data is used for training. Nor do we claim that this is a complete discussion of the ID3 algorithm, let alone classification models. 

On the contrary, the only reason why we chose this example is to provide a simple to grasp application of the `data.tree` package, in order to demonstrate how easy it is to build hierarchic models with it.

# Training

As mentioned above, when predicting, each node routes our mushroom according to an attribute? But how do we chose the attribute? That is where classification models differ. In ID3, we pick at each node the attribute with the highest *Information Gain*. In a nutshell, this is the attribute which splits the sample in the possibly *purest* subsets. For example, in the case of mushrooms, "dots" might be a more sensible attribute then "organic". 

## Purity

We define a subset to be completely pure if it contains only a *single* class. For example, if a subset contains only poisonous mushrooms, it is completely pure. In R, assuming that the last column contains the variable to be predicted, this can be written as:

```{r}
IsPure <- function(data) {
  length(unique(data[,ncol(data)])) == 1
}
```


## Attribute Selection: Entropy and Information Gain

Mathematically, the information gain IG is defined as:

$$ IG(T,a) = H(T)-\sum_{v\in vals(a)}\frac{|\{\textbf{x}\in T|x_a=v\}|}{|T|} \cdot H(\{\textbf{x}\in T|x_a=v\}) $$

So, let's rewrite that in R:

Again, the InformationGain of an attribute measures the purity we achieve by splitting. More
precisely, we measure the difference between the entropy before the split, and the weighted sum of the entropies after the split:

```{r}

Entropy <- function( vls ) {
  res <- vls/sum(vls) * log2(vls/sum(vls))
  res[vls == 0] <- 0
  -sum(res)
}


InformationGain <- function( tble ) {
  tble <- as.data.frame.matrix(tble)
  entropyBefore <- Entropy(colSums(tble))
  s <- rowSums(tble)
  entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
  informationGain <- entropyBefore - entropyAfter
  return (informationGain)
}
```


For example, using the `mushroom` data set:

```{r}
library(data.tree)
data(mushroom)
tble <- table(mushroom[,c('color', 'edibility')])
tble
InformationGain(tble)
InformationGain(table(mushroom[,c('size', 'edibility')]))
InformationGain(table(mushroom[,c('points', 'edibility')]))
```


# The ID3 Algorithm

## Pseudo Code

We are all set for the ID3 training algorithm. We start with the entire training data, and with a root. Then:

1. if the data-set is pure (e.g. all toxic), then  
    1. construct a leaf having the name of the pure attribute (e.g. 'toxic')
2. else  
    1. chose the attribute with the highest information gain (e.g. 'color')
    2. for each value of that attribute (e.g. 'red', 'brown', 'green')
        1. take the subset of the data-set having that attribute value
        2. construct a child having the name of that attribute value (e.g. 'red')
        3. call the algorithm recursively on the child and the subset

## Implementation in R with the data.tree package

For the following implementation, we assume that the classifying attributes are in columns 1 to n-1, whereas the variable to predict (the edibility) is in the last column.
```{r}
TrainID3 <- function(node, data) {
    
  node$obsCount <- nrow(data)
  
  #if the data-set is pure (e.g. all toxic), then
  if (IsPure(data)) {
    #construct a leaf having the name of the pure attribute (e.g. 'toxic')
    child <- node$AddChild(unique(data[,ncol(data)]))
    node$attribute <- tail(names(data), 1)
    child$obsCount <- nrow(data)
    child$attribute <- ''
  } else {
    #chose the attribute with the highest information gain (e.g. 'color')
    ig <- sapply(colnames(data)[-ncol(data)], 
            function(x) InformationGain(
              table(data[,x], data[,ncol(data)])
              )
            )
    attribute <- names(ig)[ig == max(ig)][1]
    
    node$attribute <- attribute
    
    #take the subset of the data-set having that attribute value
    childObs <- split(data[,!(names(data) %in% attribute)], data[,attribute], drop = TRUE)
    
    for(i in 1:length(childObs)) {
      #construct a child having the name of that attribute value (e.g. 'red')
      child <- node$AddChild(names(childObs)[i])
      
      #call the algorithm recursively on the child and the subset      
      TrainID3(child, childObs[[i]])
    }
    
  }
  
  

}
```

## Training with Data

We are ready to run the function:

```{r}
tree <- Node$new("mushroom")
TrainID3(tree, mushroom)
print(tree, "attribute", "obsCount")

```


# Prediction

## The Prediction Method

Now, let's predict some variables. For this, we need a predict function, which will route data through our tree:

```{r}

Predict <- function(tree, attributes) {
  if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
  child <- tree$children[[attributes[[tree$attribute]]]]
  return ( Predict(child, attributes))
}

```

## Using the Prediction Method

And now we use it to predict:

```{r}
Predict(tree, c(color = 'red', 
                size = 'large', 
                points = 'yes')
        )
```

Oops! Maybe _organic_ wasn't such a bad predictor, after all :-)