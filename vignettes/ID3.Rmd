---
title: "ID3 Classification using data.tree"
author: "Christoph Glur"
date: '`r Sys.Date()`'
output: 
  html_document:
    theme: united
    toc: yes
    toc_depth: 3
---

<!--
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Example of using data.tree for ID3 classification}
-->

![id3](assets/collage.jpg)

# About this Vignette

This vignette provides a real-world example of the capabilities of the data.tree package. Note that 
the data.tree package is by no means optimised for speed. Quite the contrary: for any specialised task,
you will probably find a faster implementation. The data.tree package is useful always when implementation speed
is more important than computation speed. This is usually the case when

* you want to develop and test a new algorithm
* when you don't expect large data sets
* when you just want to play around with data
* when you want to test another package, to compare it with your own results
* when you need to do homework

For example, the code for this vignette was written in an hour. The reason for this is that, thanks to the data.tree package,
the implementation of the training algorithm follows almost line by line the algorithm's pseudo code.

# The ID3 algorithm

You will find lots of documentation on the internet. 

For example:

http://www.uni-weimar.de/medien/webis/teaching/lecturenotes/machine-learning/unit-en-decision-trees-algorithms.pdf

# Entropy and Information Gain

```{r}
library(data.tree)
data(mushroom)
```

In ID3, the decision criteria is the informationGain, which measures the difference in purity we achieve by splitting. More
precisely, we measure the difference between the entropy before the split, and the weighted sum of the entropies after the split:

```{r}

entropy <- function( vls ) {
  res <- vls/sum(vls) * log2(vls/sum(vls))
  res[vls == 0] <- 0
  -sum(res)
}


informationGain <- function( tble ) {
  tble <- as.data.frame.matrix(tble)
  entropyBefore <- entropy(colSums(tble))
  s <- rowSums(tble)
  entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = entropy ))
  informationGain <- entropyBefore - entropyAfter
  return (informationGain)
}
```


For example:

```{r}
tble <- table(mushroom[,c('color', 'edibility')])
tble
informationGain(tble)
informationGain(table(mushroom[,c('size', 'edibility')]))
informationGain(table(mushroom[,c('points', 'edibility')]))
```

```{r}
isPure <- function(data) {
  length(unique(data[,ncol(data)])) == 1
}
```


# The ID3 Algorithm

## Pseudo Code

We are all set for the ID3 algorithm:

1. if the data-set is pure (e.g. all toxic), then  
    1. construct a leaf having the name of the pure attribute (e.g. 'toxic')
2. else  
    1. chose the attribute with the highest information gain (e.g. 'color')
    2. for each value of that attribute (e.g. 'red', 'brown', 'green')
        1. take the subset of the data-set having that attribute value
        2. construct a child having the name of that attribute value (e.g. 'red')
        3. call the algorithm recursively on the child and the subset

## Implementation with the data.tree package

```{r}


trainID3 <- function(node, data) {
  
  #if the data-set is pure (e.g. all toxic), then
  if (isPure(data)) {
    #construct a leaf having the name of the pure attribute (e.g. 'toxic')
    child <- node$AddChild(unique(data[,ncol(data)]))
    node$attribute <- tail(names(data), 1)
  } else {
    #chose the attribute with the highest information gain (e.g. 'color')
    ig <- sapply(colnames(data)[-ncol(data)], 
            function(x) informationGain(
              table(data[,x], data[,ncol(data)])
              )
            )
    attribute <- names(ig)[ig == max(ig)][1]
    
    node$attribute <- attribute
    
    #take the subset of the data-set having that attribute value
    childObs <- split(data[,!(names(data) %in% attribute)], data[,attribute], drop = TRUE)
    
    for(i in 1:length(childObs)) {
      #construct a child having the name of that attribute value (e.g. 'red')
      child <- node$AddChild(names(childObs)[i])
      #call the algorithm recursively on the child and the subset      
      trainID3(child, childObs[[i]])
    }
    
  }
  
  

}
```

# Training with Data

We are ready to run the function:

```{r}
tree <- Node$new("mushroom")
trainID3(tree, mushroom)
print(tree, "attribute")

```

Another example is the well-known plaiying tennis example:

```{r}
data(ball)
ballTree <- Node$new("ball")
trainID3(ballTree, ball)
print(ballTree, "attribute")

```

Now, let's predict some variables. For this, we need a predict function, which will route data through our tree:

```{r}

predict <- function(tree, attributes) {
  if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
  child <- tree$children[[attributes[[tree$attribute]]]]
  return ( predict(child, attributes))
}

```

And now we use it to predict:

```{r}
predict(ballTree, c(outlook = 'sunny', 
                    temperature = 'mild', 
                    humidity = 'high', 
                    wind = 'strong')
        )
```

So, let's compare the prediction with the actual values, as a sorts of double-check:

```{r}

ball$predict <- apply(ball, MARGIN = 1, FUN = function(x) predict(ballTree, x))

```

Looking pretty good!
